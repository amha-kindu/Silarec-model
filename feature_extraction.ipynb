{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.8\n",
    ")\n",
    " \n",
    "# Initializing the drawng utils for drawing the landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(results):\n",
    "    POSE_LANDMARK_COUNT, HAND_LANDMARK_COUNT, FACE_LANDMARK_COUNT = 33, 21, 468\n",
    "    pose_landmarks = [0 for _ in range(POSE_LANDMARK_COUNT * 4)]\n",
    "    left_hand_landmarks = [0 for _ in range(HAND_LANDMARK_COUNT * 3)]\n",
    "    right_hand_landmarks = [0 for _ in range(HAND_LANDMARK_COUNT * 3)]\n",
    "    # face_landmarks = [0 for _ in range(FACE_LANDMARK_COUNT * 3)]\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        pose_landmarks = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            pose_landmarks += [landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_landmarks = []\n",
    "        for landmark in results.left_hand_landmarks.landmark:\n",
    "            left_hand_landmarks += [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_landmarks = []\n",
    "        for landmark in results.right_hand_landmarks.landmark:\n",
    "            right_hand_landmarks += [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "    # if results.face_landmarks:\n",
    "    #     face_landmarks = []\n",
    "    #     for landmark in results.face_landmarks.landmark:\n",
    "    #         face_landmarks += [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "    return pose_landmarks+left_hand_landmarks+right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_landmarks(image, results):\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#       image,\n",
    "#       results.pose_landmarks,\n",
    "#       mp_holistic.POSE_CONNECTIONS,\n",
    "#       mp_drawing.DrawingSpec(\n",
    "#         color=(255,0,255),\n",
    "#         thickness=2,\n",
    "#         circle_radius=2\n",
    "#       ),\n",
    "#       mp_drawing.DrawingSpec(\n",
    "#         color=(255,255,255),\n",
    "#         thickness=2,\n",
    "#         circle_radius=2\n",
    "#       )\n",
    "#     )\n",
    " \n",
    "#     # Drawing Right hand Land Marks\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#       image,\n",
    "#       results.right_hand_landmarks,\n",
    "#       mp_holistic.HAND_CONNECTIONS\n",
    "#     )\n",
    " \n",
    "#     # Drawing Left hand Land Marks\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#       image,\n",
    "#       results.left_hand_landmarks,\n",
    "#       mp_holistic.HAND_CONNECTIONS\n",
    "#     )\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANUAL-TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data of 'africa'...\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import clear_output\n",
    "# TRAINING_DATA_PATH = r'..\\dataset\\WLASL\\start_kit\\videos\\train'\n",
    "# FRAME_CNT = 50\n",
    "# word_map = {}\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for gloss in os.listdir(TRAINING_DATA_PATH)[:3]:\n",
    "#     word_map[len(word_map)+1] = gloss\n",
    "#     word_path = os.path.join(TRAINING_DATA_PATH, gloss)\n",
    "\n",
    "#     clear_output(wait=True)\n",
    "#     print('Extracting data of \\'{}\\'...'.format(gloss))\n",
    "\n",
    "#     for variation  in os.listdir(word_path):\n",
    "#         variation_path = os.path.join(word_path, variation)\n",
    "        \n",
    "#         cap = cv.VideoCapture(variation_path)\n",
    "#         total_frames = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "        \n",
    "#         if total_frames<FRAME_CNT:\n",
    "#             cap.release()\n",
    "#             continue\n",
    "\n",
    "#         frames_step = total_frames // FRAME_CNT\n",
    "        \n",
    "#         i = 10\n",
    "#         variation_landmarks = []\n",
    "#         buggy = False\n",
    "#         while i<FRAME_CNT and i<total_frames:\n",
    "#             #Set the frame number to (i*frames_step)\n",
    "#             cap.set(1,  i*frames_step)\n",
    "#             success, image = cap.read() \n",
    "#             if not success:\n",
    "#                 buggy = True\n",
    "#                 break\n",
    "#             image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "#             # retrieve landmarks using holistic model\n",
    "#             image.flags.writeable = False\n",
    "#             results = holistic_model.process(image)\n",
    "#             image.flags.writeable = True\n",
    "\n",
    "#             draw_landmarks(image, results)\n",
    "\n",
    "#             cv.imwrite('test_'+gloss+str(i)+'.jpg', image)\n",
    "        \n",
    "#             variation_landmarks.append(get_landmarks(results))\n",
    "#             i+=1\n",
    "        \n",
    "#         cap.release()\n",
    "#         if not buggy:\n",
    "#             X.append(variation_landmarks)\n",
    "#             y.append(gloss)\n",
    "# # cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data of 'your'...\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "TESTING_DATA_PATH = r'..\\dataset\\WLASL\\start_kit\\videos\\test'\n",
    "FRAME_CNT = 50\n",
    "word_map = {}\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for gloss in os.listdir(TESTING_DATA_PATH):\n",
    "    word_map[len(word_map)+1] = gloss\n",
    "    word_path = os.path.join(TESTING_DATA_PATH, gloss)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f'Extracting data of \\'{gloss}\\'...')\n",
    "\n",
    "    for variation  in os.listdir(word_path):\n",
    "        variation_path = os.path.join(word_path, variation)\n",
    "        \n",
    "        cap = cv.VideoCapture(variation_path)\n",
    "        total_frames = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "        \n",
    "        if total_frames<FRAME_CNT:\n",
    "            cap.release()\n",
    "            continue\n",
    "\n",
    "        frames_step = total_frames // FRAME_CNT\n",
    "        \n",
    "        i = 10\n",
    "        variation_landmarks = []\n",
    "        buggy = False\n",
    "        while i<FRAME_CNT and i<total_frames:\n",
    "            \n",
    "            #Set the frame number to (i*frames_step)\n",
    "            cap.set(1,  i*frames_step)\n",
    "            success, image = cap.read() \n",
    "            if not success:\n",
    "                buggy = True\n",
    "                break\n",
    "            \n",
    "            image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "            # retrieve landmarks using holistic model\n",
    "            image.flags.writeable = False\n",
    "            results = holistic_model.process(image)\n",
    "            image.flags.writeable = True\n",
    "        \n",
    "            variation_landmarks.append(get_landmarks(results))\n",
    "            i+=1\n",
    "        \n",
    "        cap.release()\n",
    "        if not buggy:\n",
    "            X.append(variation_landmarks)\n",
    "            y.append(gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X=np.array(X)       \n",
    "\n",
    "y = np.array(y)\n",
    "words_encoded = y.reshape(y.size, 1)\n",
    "y = OneHotEncoder(sparse=False).fit_transform(words_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 40, 258)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(296, 233)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_233_Test_batch.npy', X)\n",
    "np.save('y_233_Test_batch.npy', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
